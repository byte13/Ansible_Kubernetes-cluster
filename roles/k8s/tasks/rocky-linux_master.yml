---
# tasks file for k8s

- name: "Show facts available on the system"
  ansible.builtin.debug:
    var: ansible_facts
  when: k8s_display_facts == "yes"

# selinux block requires to install a module using "ansible-galaxy collection install ansible.posix"
- name: "Set selinux to permissive mode during installation"
  ansible.posix.selinux:
    policy: targeted
    state: permissive
  when: centos_selinux == "yes"

- name: "Set firewall rules on k8s master nodes on CentOS/RedHat/Rocky-Linux"
  firewalld:
    zone: public
    port: "{{ item }}"
    permanent: yes
    immediate: yes
    state: enabled
  loop:
    - 2379-2380/tcp
    - 6443/tcp
    - 10250/tcp
    - 10251/tcp
    - 10252/tcp
    - 10255/tcp
    - 10257/tcp
    - 10259/tcp

  when: (inventory_hostname in groups['k8smasternodes']) and (global_cni != 'calico')
  notify:
    - restart firewalld

# Stop and disable firewalld; use Calico to manage IP filtering on Kubernetes nodes interfaces
# References: https://docs.tigera.io/calico/latest/getting-started/kubernetes/requirements
#             https://docs.tigera.io/calico/latest/network-policy/hosts
- name: Stop and disable firewalld; use Calico to manage IP filtering on Kubernetes nodes interfaces
  ansible.builtin.service:
    name: firewalld
    state: stopped
    enabled: no
  when: (inventory_hostname in groups['k8smasternodes']) and (global_cni == 'calico')

- name: "Disable memory swapping to disk since kubernetes cannot work with swap enabled"
  shell: |
    swapoff -a
    # when: kubernetes_installed is changed

- name: "Disable memory swapping to disk in fstab since kubernetes cannot work with swap enabled"
  replace:
    path: /etc/fstab
    #regexp: '^([^#].*?\sswap\s+sw\s+.*)$'
    regexp: '^(\/dev\/mapper\/rl-swap)'
    replace: '# \1'
  when: ansible_facts['os_family']|lower == 'redhat'

- name: "Disable memory swapping to disk in fstab since kubernetes cannot work with swap enabled"
  replace:
    path: /etc/fstab
    #regexp: '^([^#].*?\sswap\s+sw\s+.*)$'
    regexp: '^(\/dev\/mapper\/rl-swap)'
    replace: '# \1'
  when: ansible_facts['os_family']|lower == 'rocky'

# Set overlay and br_netfilter kernel modules to be loaded on system startup 
- name: "Set overlay and br_netfilter kernel modules to be loaded on system startup"
  lineinfile:
    dest: /etc/modules-load.d/crio.conf 
    line: "{{ item }}"
    state: present
    create: yes
    backup: yes
  loop:
    - overlay
    - br_netfilter

# Load overlay and br_netfilter kernel modules
- name: "Activate overlay and br_netfilter kernel modules"
  modprobe:
    name: "{{ item }}"
    state: present
  loop:
    - overlay
    - br_netfilter

- name: "Stop kubelet"
  systemd:
    name: kubelet
    state: stopped
  ignore_errors: true
  when: k8s_reset_cluster == "yes"

- name: "Uninstall Kubernetes packages: {{ centos_kubeadm_pkgname }}, {{ centos_kubelet_pkgname }}, {{ centos_kubectl_pkgname }}"
  dnf:
    name:
      - "{{ centos_critools_pkgname }}"
      - "{{ centos_kubeadm_pkgname }}"
      - "{{ centos_kubelet_pkgname }}"
      - "{{ centos_kubectl_pkgname }}"
      - "{{ centos_kubeadm_pkgname }}-{{ k8s_version }}"
      - "{{ centos_kubelet_pkgname }}-{{ k8s_version }}"
      - "{{ centos_kubectl_pkgname }}-{{ k8s_version }}"
    state: absent
    update_cache: yes
  when: k8s_reset_cluster == "yes"

- name: Recursively remove /etc/kubernetes and /var/lib/kubelet
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes
    - /var/lib/kubelet
  when: k8s_reset_cluster == "yes"

# Note that Kubernetes dnf/yum repository has been added by dependencies role
# crictl is a CLI by Kubernetes to interact with installed CRI; it is required to deploy Kubernetes with kubeadm
- name: "Install Kubernetes packages: {{ centos_tc_pkgname }}, {{ centos_nettools_pkgname }},{{ centos_kubeadm_pkgname }}, {{ centos_kubelet_pkgname }}, {{ centos_kubectl_pkgname }}"
  dnf:
    name:
      - "{{ centos_tc_pkgname }}"
      - "{{ centos_nettools_pkgname }}"
      - "{{ centos_critools_pkgname }}"
      - "{{ centos_kubeadm_pkgname }}-{{ k8s_version }}"
      - "{{ centos_kubelet_pkgname }}-{{ k8s_version }}"
      - "{{ centos_kubectl_pkgname }}-{{ k8s_version }}"
    state: present
    update_cache: yes
  notify:
    - enable and start kubelet

# Reset Kubernetes cluster using kubeadm
- name: "Possibly reset cluster (set k8s_reset_cluster variable to 'no' to not re-initialize the cluster)"
  command: kubeadm reset -f
  when: k8s_reset_cluster == "yes"

# Upload kubelet arguments configuration file 
- name: "Upload kubelet arguments configuration file"
  template:
    src:   kubelet.j2
    dest:  "{{ k8s_kubelet_extraargs_destfile }}"
    owner: root
    group: root
    mode:  0640
  notify:
    - reload systemd
 
- name: Initialize Kubernetes cluster using kubeadm
  command: kubeadm init --apiserver-advertise-address={{ k8s_apiserver_advip }} \
                        --service-cidr={{ k8s_services_subnet }} \
                        --pod-network-cidr={{ k8s_pods_subnet }} \
                        --upload-certs
  register: kubernetes_kubeadminit_output
  when: inventory_hostname in groups['k8smasternodes']

#- debug:
#  msg: "{{ kubernetes_join_worker_command.stdout }}"
- name: "Save kubeadm init output in local {{ k8s_join_dir }}/{{ inventory_hostname }}_{{ k8s_kubeadminit_outputfile }}"
  become: no
  local_action:
    copy content="{{ kubernetes_kubeadminit_output }}" dest="{{ k8s_join_dir }}/{{ inventory_hostname }}_{{ k8s_kubeadminit_outputfile }}" mode="0600"
  when: inventory_hostname in groups['k8smasternodes'] 

- name: "Create ~{{ remote_user }}/.kube directory on target node"
  file:
    path: ~{{ remote_user }}/.kube
    state: directory
    owner: "{{ remote_user }}"
    mode: 0750

- name: "Copy /etc/kubernetes/admin.conf to ~{{ remote_user }}/.kube/config on target node"
  copy:
    src: /etc/kubernetes/admin.conf
    dest: ~{{ remote_user }}/.kube/config
    remote_src: yes
    backup: yes
    owner: "{{ remote_user }}"
    mode: 0600

# Get token to join workers to the cluster 
- name: "Get token to join workers to the cluster"
  become: no
  shell: kubeadm token create --print-join-command
  register: kubernetes_join_worker_command
  when: inventory_hostname in groups['k8smasternodes'] 

#- debug:
#  msg: "{{ kubernetes_join_worker_command.stdout }}"
# Save join command for workers in local file
- name: "Save join command for workers in local {{ k8s_join_dir }}/{{ inventory_hostname }}_{{ k8s_join_worker_file }}"
  become: no
  local_action:
    copy content="{{ kubernetes_join_worker_command.stdout_lines[0] }}" dest="{{ k8s_join_dir }}/{{ inventory_hostname }}_{{ k8s_join_worker_file }}" mode="0600"
  when: inventory_hostname in groups['k8smasternodes'] 

# Create soft link to join command file to be used later on worker nodes 
- name: "Create soft link to {{ k8s_join_workers_dir }}/{{ inventory_hostname }}_{{ k8s_join_workers_file }}"
  become: no
  local_action:
    command ln -s -f {{ inventory_hostname }}_{{ k8s_join_worker_file }} {{ k8s_join_worker_file }}
  when: inventory_hostname in groups['k8smasternodes'] 

# Retrieve kubeconfig file under {{ k8s_kubeconfig_dir }}
- name: "Retrieve kubeconfig file under {{ k8s_kubeconfig_dir }}. To manage the cluster, use kubectl --kubeconfig={{ inventory_hostname }}_admin.conf ..."
  fetch:
    src: /etc/kubernetes/admin.conf
    dest: "{{ k8s_kubeconfig_dir }}/{{ inventory_hostname }}_admin.conf"
    flat: yes
    validate_checksum: yes
  when: inventory_hostname in groups['k8smasternodes'] 

- name: "Restart kubelet" 
  service:
    name: kubelet 
    state: restarted
    enabled: yes

#- name: "Wait for k8s master node(s) to be ready..."
#  become: no
#  shell: |
#    kubectl wait --for=condition=Ready nodes --selector node-role.kubernetes.io/control-plane="" --timeout=180s
#  register: masternodes_ready
#  when: (inventory_hostname in groups['k8smasternodes']) and (global_wait_masters == "yes")

#- name: "Wait for all pods on k8s master node(s) to be ready..."
#  become: no
#  shell: |
#    kubectl wait pod --all -n kube-system --for condition=Ready --timeout=600s 
#  register: masternodes_containers_ready
#  when: (inventory_hostname in groups['k8smasternodes']) and (global_wait_masters == "yes")

- name: "Set selinux back to mode defined in centos_selinux_state variable in role/k8s/vars/main.yml (current value : {{ centos_selinux_state }})"
  ansible.posix.selinux:
    policy: targeted
    state: "{{ centos_selinux_state }}"
  when: centos_selinux == "yes"

#- name: "Ensure selinux remains enforcing accross reboots after initialization of Kubernetes on" 
#  replace:
#  path: /etc/sysconfig/selinux 
#  regexp: '^([^#].*?\sSELINUX=permissive\s+sw\s+.*)$'
#  replace: 'SELINUX=enforcing'
#  when: centos_selinux == "yes"
